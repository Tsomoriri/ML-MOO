{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base, creator, tools, algorithms\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deap'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# DataFrame 'df' with columns: 'WineType', 'Alcohol', 'AshAlkalinity', etc.\n",
    "\n",
    "df = pd.read_csv('wine.csv')\n",
    "X = df.drop(columns=['ash', 'proline'])\n",
    "y = df['alcohol']\n",
    "\n",
    "# Convert the target variable to integer type\n",
    "y = y.astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    num_boost_round, max_depth, min_child_weight = individual\n",
    "    min_child_weight = max(0.0000000001, min_child_weight)  # Ensure min_child_weight is greater than or equal to 0\n",
    "    num_boost_round = int(num_boost_round)\n",
    "    max_depth = max(0,int(max_depth))  # XGBoost requires integer values for these parameters\n",
    "    # Define XGBoost hyperparameters\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',  # Example: For multiclass classification\n",
    "        'num_class': 178,  # Number of classes in your problem\n",
    "        'booster': 'gbtree',  # 'gbtree' or 'gblinear'\n",
    "        'lambda': 1,  # Regularization term\n",
    "        'min_child_weight': int(min_child_weight),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "        'subsample': 0.8,  # Subsample ratio of the training instance\n",
    "        'colsample_bylevel': 0.8,  # Subsample ratio of columns for each level\n",
    "        'colsample_bytree': 0.8,  # Subsample ratio of columns for each tree\n",
    "        'learning_rate': 0.1,  # Step size shrinkage to prevent overfitting\n",
    "        'num_boost_round': int(num_boost_round),  # Number of boosting rounds\n",
    "        'max_depth': int(max_depth),  # Maximum depth of a tree\n",
    "        # Add more XGBoost parameters as needed\n",
    "    }\n",
    "\n",
    "    # ...\n",
    "\n",
    "    min_child_weight = int(min_child_weight)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(dtest)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "    # Calculate macro average\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    macro_avg = report['macro avg']['f1-score']\n",
    "\n",
    "    return accuracy, macro_avg\n",
    "\n",
    "# Define the optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, 1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"num_boost_round\", random.randint, 1, 100)\n",
    "toolbox.register(\"max_depth\", random.randint, 1, 10)\n",
    "toolbox.register(\"min_child_weight\", random.uniform, 1, 10)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.num_boost_round, toolbox.max_depth, toolbox.min_child_weight), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "# Run the optimization algorithm (NSGA-II)\n",
    "def main():\n",
    "    # Suppress runtime warnings and user warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    # Store the values of num_boost_round, min_child_weight, and accuracy\n",
    "    num_boost_round_values = []\n",
    "    min_child_weight_values = []\n",
    "    accuracy_values = []\n",
    "    pop = toolbox.population(n=14)\n",
    "    algorithms.eaMuPlusLambda(pop, toolbox, mu=14, lambda_=4, cxpb=0.7, mutpb=0.2, ngen=5, stats=None, halloffame=None, verbose=True)\n",
    "\n",
    "    # Get the best individual(s) found by NSGA-II\n",
    "    best_individuals = tools.sortNondominated(pop, len(pop), first_front_only=False)[0]\n",
    "    print('best set of hyperparameters',best_individuals)\n",
    "    for individual in best_individuals:\n",
    "            num_boost_round_values.append(individual[0])\n",
    "            min_child_weight_values.append(individual[2])\n",
    "            accuracy_values.append(evaluate(individual)[0])\n",
    "    # Extract the hyperparameters of the best individual\n",
    "    best_hyperparameters = best_individuals[0]\n",
    "    best_hyperparameters = best_hyperparameters  # Assuming only one best individual, adjust if multiple\n",
    "    print('best hyperparameters',best_hyperparameters)\n",
    "    # Evaluate the best individual\n",
    "    best_accuracy, best_macro_avg = evaluate(best_hyperparameters)\n",
    "\n",
    "    # Print the best hyperparameters and their performance\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(\"Num Boost Round:\", best_hyperparameters[0])\n",
    "    print(\"Max Depth:\", best_hyperparameters[1])\n",
    "    print(\"Min Child Weight:\", best_hyperparameters[2])\n",
    "    print(\"Best Accuracy:\", best_accuracy)\n",
    "    print(\"Best Macro Average:\", best_macro_avg)\n",
    "\n",
    "    # Plot the Pareto optimal parameters\n",
    "    pareto_front = np.array([indiv.fitness.values for indiv in best_individuals])\n",
    "\n",
    "    plt.scatter(pareto_front[:, 0], pareto_front[:,1])\n",
    "    plt.xlim(0, 0.999)  # Set the x-axis limits from 0 to 1\n",
    "    plt.ylim(0, 0.999)  # Set the y-axis limits from 0 to 1\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "    plt.ylabel(\"Macro Average\")\n",
    "    plt.title(\"Pareto Optimal Parameters\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the change in num_boost_round, min_child_weight, and accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(num_boost_round_values, label='Num Boost Round')\n",
    "    plt.plot(min_child_weight_values, label='Min Child Weight')\n",
    "    plt.plot(accuracy_values, label='Accuracy')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Change in Num Boost Round, Min Child Weight, and Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
