{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "# Read the train data from the wine.csv file\n",
    "train_data = TabularDataset('wine.csv')\n",
    "\n",
    "# Define the multi-objective optimization problem\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "# Define the search space (decision variables)\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"lambda_param\", random.uniform, 1e-4, 1e-1)\n",
    "toolbox.register(\"min_child_weight\", random.uniform, 1, 10)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.lambda_param, toolbox.min_child_weight), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(individual):\n",
    "    lambda_param, min_child_weight = individual\n",
    "    \n",
    "    # Train and evaluate the AutoGluon model with the given hyperparameters\n",
    "    hyperparameter_space = {\n",
    "        'model': 'XGB',\n",
    "        'hyperparameters': {\n",
    "            'lambda': lambda_param,\n",
    "            'min_child_weight': min_child_weight,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    predictor = TabularPredictor(label='WineType').fit(train_data, hyperparameters=hyperparameter_space)\n",
    "    \n",
    "    # Obtain the model performance metrics\n",
    "    accuracy = predictor.evaluate(valid_data)['accuracy']\n",
    "    \n",
    "    # Objective: Maximize accuracy, Minimize training time (placeholder; you can adjust based on your goals)\n",
    "    training_time = 1.0\n",
    "    \n",
    "    return accuracy, training_time\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "# Define the optimization algorithm (NSGA-II)\n",
    "def main():\n",
    "    pop = toolbox.population(n=50)\n",
    "    algorithms.eaMuPlusLambda(pop, toolbox, mu=50, lambda_=100, cxpb=0.7, mutpb=0.2, ngen=50, stats=None, halloffame=None, verbose=True)\n",
    "\n",
    "    # Get the best individual(s) found by NSGA-II\n",
    "    best_individuals = tools.sortNondominated(pop, len(pop), first_front_only=True)[0]\n",
    "\n",
    "    # Use the best hyperparameters to train the final AutoGluon model\n",
    "    best_hyperparameters = [indiv[0] for indiv in best_individuals]\n",
    "    best_hyperparameters = best_hyperparameters[0]  # Assuming only one best individual, you may need to adjust if multiple\n",
    "\n",
    "    hyperparameter_space = {\n",
    "        'model': 'XGB',\n",
    "        'hyperparameters': {\n",
    "            'lambda': best_hyperparameters[0],\n",
    "            'min_child_weight': best_hyperparameters[1],\n",
    "        }\n",
    "    }\n",
    "\n",
    "    final_predictor = TabularPredictor(label='WineType').fit(train_data, hyperparameters=hyperparameter_space)\n",
    "    \n",
    "    # You can save or use the final_predictor for making predictions on new data\n",
    "    final_predictions = final_predictor.predict(new_data)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "from autogluon.tabular.models import XGBoostModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the wine dataset\n",
    "dataset = TabularDataset('wine.csv')\n",
    "\n",
    "# Define the features and target column\n",
    "features = ['alcohol', 'ash', 'proline']\n",
    "target = 'alcohol'\n",
    "\n",
    "# Define the standard hyperparameters for AutoGluon\n",
    "standard_hyperparameters = {\n",
    "    'model': 'XGB',\n",
    "    'hyperparameters': {\n",
    "        'lambda': 1e-4,\n",
    "        'min_child_weight': 1,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train the initial AutoGluon model\n",
    "initial_predictor = TabularPredictor(label=target).fit(dataset, hyperparameters=standard_hyperparameters)\n",
    "\n",
    "# Run NSGA-II to find the best hyperparameters\n",
    "creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"lambda_param\", random.uniform, 1e-4, 1e-1)\n",
    "toolbox.register(\"min_child_weight\", random.uniform, 1, 10)\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.lambda_param, toolbox.min_child_weight), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "def evaluate(individual):\n",
    "    lambda_param, min_child_weight = individual\n",
    "    \n",
    "    hyperparameters = {\n",
    "        'model': 'XGB',\n",
    "        'hyperparameters': {\n",
    "            'lambda': lambda_param,\n",
    "            'min_child_weight': min_child_weight,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    predictor = TabularPredictor(label=target).fit(dataset, hyperparameters=hyperparameters)\n",
    "    accuracy = predictor.evaluate(dataset)['accuracy']\n",
    "    \n",
    "    training_time = 1.0\n",
    "    \n",
    "    return accuracy, training_time\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selNSGA2)\n",
    "\n",
    "def main():\n",
    "    pop = toolbox.population(n=50)\n",
    "    TabularPredictor.register_model(model_type='XGB', model=XGBoostModel)\n",
    "\n",
    "    return initial_predictor, final_predictor\n",
    "\n",
    "# Register XGBoost as a custom model preset\n",
    "TabularPredictor.register_model(model_type='XGB', model=XGBoostModel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LEGEON\\anaconda3\\envs\\moo\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [23:15:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0750514818a16474a-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"num_boost_round\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 2  0  0  0]\n",
      " [ 0 13  0  0]\n",
      " [ 0  0 17  0]\n",
      " [ 0  0  3  1]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          11       1.00      1.00      1.00         2\n",
      "          12       1.00      1.00      1.00        13\n",
      "          13       0.85      1.00      0.92        17\n",
      "          14       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.92        36\n",
      "   macro avg       0.96      0.81      0.83        36\n",
      "weighted avg       0.93      0.92      0.90        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Assuming you have a DataFrame 'df' with columns: 'WineType', 'Alcohol', 'AshAlkalinity', etc.\n",
    "# Replace 'WineType' with the actual target variable and other columns with your features\n",
    "df = pd.read_csv('wine.csv')\n",
    "X = df.drop(columns=['ash', 'proline'])\n",
    "y = df['alcohol']\n",
    "\n",
    "# Convert the target variable to integer type\n",
    "y = y.astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define XGBoost hyperparameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Example: For multiclass classification\n",
    "    'num_class': 178,  # Number of classes in your problem\n",
    "    'booster': 'gbtree',  # 'gbtree' or 'gblinear'\n",
    "    'lambda': 1,  # Regularization term\n",
    "    'min_child_weight': 1,  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'subsample': 0.8,  # Subsample ratio of the training instance\n",
    "    'colsample_bylevel': 0.8,  # Subsample ratio of columns for each level\n",
    "    'colsample_bytree': 0.8,  # Subsample ratio of columns for each tree\n",
    "    'learning_rate': 0.1,  # Step size shrinkage to prevent overfitting\n",
    "    'num_boost_round': 100,  # Number of boosting rounds\n",
    "    'max_depth': 1,  # Maximum depth of a tree\n",
    "    # Add more XGBoost parameters as needed\n",
    "}\n",
    "\n",
    "# Convert data to DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = xgb.train(params, dtrain, num_boost_round=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Create a classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
